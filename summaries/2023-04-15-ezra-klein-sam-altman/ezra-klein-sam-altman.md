---
date: 2023-04-15
type: interview
summary: Ezra Klein interview with Sam Altman
url: https://www.nytimes.com/2021/06/11/podcasts/transcript-ezra-klein-interviews-sam-altman.html
---

This is an interview from way back in 2021 that I happened to listen to
recently. It's an interesting conversation to listen to _now_, given how rapidly
the AI landscape has changed since the [release of ChatGPT][1].

I actually listened to this interview about a week ago, so my memory of it is
pretty limited. I will just cover two main aspects of the conversation that have
stuck with me.

Klein begins by referencing an article by Altman called [Moore's Law for
Everything][2]. (Hello, reading list!) The basic idea behind this is that AGI
could lead to progress in almost countless dimensions in the same way that
[Moore's Law][3] famously projects steady, predictable progress in transistor
technology (the number of transistors in an integrated circuit doubles about
every two years).

This is an illuminating glimpse into Altman's mind, suggesting one of the
primary ideas motivating him to want to introduce AGI to the world. I must add
this it is also peculiar to me how at one point in the interview Altman talks
about how there is "no upper limit" to what AGI should be able to achieve. I
find this peculiar because it seems to me that Moore's law _does_ have an upper
limit, namely the physical limits of how small a transistor could possibly be
(I'm no expert here, but surely no smaller than an atom?). And from what I know,
much of the progress in AI research to date has come from doubling the resources
dedicated to producing large language models (LLMs). I feel like if you are
doubling your investment at every major milestone, once again surely there is
some practical upper limit, even if it is very far away. (For example, you can
only build as many GPUs as there is enough of the necessary materials on planet
Earth.)

The other thing I remember from the conversation is that Klein raised concerns
throughout regarding the distribution of power. He wondered how people like
Altman could ensure that the immense value unlocked by AGI benefits humanity as
a whole rather than only a small, privileged circle. I recall that Altman
acknowledged this concern and expressed the belief that government regulation
will be needed.

An interesting little debate happened around the middle of the interview, when
Altman said he didn't see any reason why a trillionaire shouldn't exist one day.
Klein pushed back on this, wondering why an individual person should need that
much wealth and power. I wouldn't say they dove particularly deep into this
question; but again, it provided an interesting glimpse into the mind of the man
who is currently leading arguably the most successful and influential AI company
in the world.

[1]: https://openai.com/blog/chatgpt
[2]: https://moores.samaltman.com/
[3]: https://en.wikipedia.org/wiki/Moore%27s_law
