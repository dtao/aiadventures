---
date: 2023-04-07
type: article
summary: "\"Our approach to AI safety\", OpenAI"
url: https://openai.com/blog/our-approach-to-ai-safety
---

I was expecting something more in-depth, but this article was basically just a
summary of the _kinds_ of things OpenAI does to try and ensure the safety of
the systems they build such as GPT-4.

They put extra care into protecting children, e.g. by reporting abusive content
to the National Center for Missing and Exploited Children, and by working
closely with partners such as Khan Academy. They try to remove personal
information from their training data. They use RLHF to flag factual
inaccuracies.

Probably the main point of the article is to explain how rolling out AI
capabilities gradually, as they're developed, is integral to the company's
strategy for ensuring AI safety:

> Crucially, we believe that society must have time to update and adjust to
> increasingly capable AI, and that  everyone who is affected by this technology
> should have a significant say in how AI develops further. Iterative deployment
> has helped us bring various stakeholders into the conversation about the
> adoption of AI technology more effectively than if they hadn't had firsthand
> experience with these tools.

This is the same point that Sam Altman made in his [Kara Swisher interview][1].

[1]: /summaries/kara-swisher-sam-altman.html
